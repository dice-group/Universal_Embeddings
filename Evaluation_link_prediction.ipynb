{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "apparent-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "attractive-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "scheduled-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Degree(file_name):\n",
    "    with open(\"Evaluation/\"+file_name+\".txt\", 'r') as file:\n",
    "        data = file.readlines()\n",
    "    degree = defaultdict(lambda: 0)\n",
    "    for triple in data:\n",
    "        e1, r, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "        degree[e1] += 1\n",
    "        degree[e2] += 1\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "clear-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(triples):\n",
    "    entities = set()\n",
    "    for l in triples:\n",
    "        e1, _, e2 = l.split(\"\\t\")\n",
    "        entities.update({e1, e2})\n",
    "    return entities\n",
    "\n",
    "def get_relations(triples):\n",
    "    relations = set()\n",
    "    for l in triples:\n",
    "        relations.add(l.split('\\t')[1])\n",
    "    return relations\n",
    "\n",
    "def filter_triples(triples, train, train_entities, train_relations):\n",
    "    remaining, removed = [], []\n",
    "    for l in triples: # Ony keep triples whose entities and relations are in the training split\n",
    "        e1, r, e2 = l.split('\\t')\n",
    "        if e1 in train_entities and e2 in train_entities and r in train_relations:\n",
    "            remaining.append(l)\n",
    "        else:\n",
    "            removed.append(l)\n",
    "    return train+removed, remaining\n",
    "\n",
    "def write_to_file(storage_path, data):\n",
    "    with open(storage_path, 'w') as file:\n",
    "        file.writelines(data)\n",
    "\n",
    "def split_data(file_name, low_degree=3):\n",
    "    with open(\"Evaluation/\"+file_name+\".txt\", 'r') as file:\n",
    "        data = file.readlines()\n",
    "        print(f\"\\nDataset size: {len(data)} triples\")\n",
    "    \n",
    "    ## Remove low degree entities\n",
    "    degrees = Degree(file_name)\n",
    "    new_data = []\n",
    "    for triple in tqdm(data, desc=\"Removing entities with degree less than 3...\"):\n",
    "        e1, _, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "        if degrees[e1] >= low_degree and degrees[e2] >= low_degree:\n",
    "            new_data.append(triple)\n",
    "    del data\n",
    "    data = new_data\n",
    "    print(f\"\\nDataset size without low degree entities: {len(data)} triples\\n\")\n",
    "    train, temp_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_entities = get_entities(train)\n",
    "    train_relations = get_relations(train)\n",
    "    \n",
    "    train, test = filter_triples(temp_test, train, train_entities, train_relations)\n",
    "    \n",
    "    print(f\"\\nStatistics train: {100*float(len(train))/len(data)}%,  test: {100*float(len(test))/len(data)}%\")\n",
    "    storage_paths = [f'Evaluation/splits/{file_name}_train.txt', f'Evaluation/splits/{file_name}_test.txt']\n",
    "    for path, d in zip(storage_paths, [train, test]):\n",
    "        write_to_file(path, d)\n",
    "    print('\\nData split completed!')\n",
    "    \n",
    "\n",
    "def split_merge(Wiki_triples_train, DBpedia_triples_train):\n",
    "    with open(f\"Evaluation/splits/{Wiki_triples_train}.txt\") as file2:\n",
    "        data_Wiki_train = file2.readlines()\n",
    "    with open(f\"Evaluation/splits/{DBpedia_triples_train}.txt\") as file4:\n",
    "        data_DBp_train = file4.readlines()\n",
    "    with open(\"Evaluation/splits/Merged_train.txt\", \"w\") as file_merge_train:\n",
    "        file_merge_train.writelines(data_Wiki_train+data_DBp_train)\n",
    "    print(\"\\n Merge split completed!\\n\")\n",
    "    print(f\"Train: {len(data_Wiki_train)+len(data_DBp_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "appropriate-dinner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 1466131 triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing entities with degree less than 3...: 100%|██████████| 1466131/1466131 [00:01<00:00, 849151.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size without low degree entities: 289575 triples\n",
      "\n",
      "\n",
      "Statistics train: 81.43451610118277%,  test: 18.56548389881723%\n",
      "\n",
      "Data split completed!\n"
     ]
    }
   ],
   "source": [
    "#split_data(\"Wikidata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distinct-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 275170 triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing entities with degree less than 3...: 100%|██████████| 275170/275170 [00:00<00:00, 606946.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size without low degree entities: 85041 triples\n",
      "\n",
      "\n",
      "Statistics train: 81.92166131630626%,  test: 18.078338683693747%\n",
      "\n",
      "Data split completed!\n"
     ]
    }
   ],
   "source": [
    "#split_data(\"DBpedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "smaller-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Merge split completed!\n",
      "\n",
      "Train: 305481\n"
     ]
    }
   ],
   "source": [
    "#split_merge(\"Wikidata_train\",\"DBpedia_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "furnished-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Evaluation/splits/DBpedia_train.txt\") as file:\n",
    "    db_train = file.readlines()\n",
    "Ents = set()\n",
    "for triple in db_train:\n",
    "    e1, r, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "    Ents.update({e1, e2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hungry-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "sameAs = list(filter(lambda x: '/entity/' in x, Ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "continuous-ownership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22102"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sameAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "humanitarian-organ",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31116"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "proof-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Evaluation/splits/Wikidata_train.txt\") as file:\n",
    "    db_train = file.readlines()\n",
    "Ents = set()\n",
    "for triple in db_train:\n",
    "    e1, r, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "    Ents.update({e1, e2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lesbian-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72058"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-madrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "governmental-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "juvenile-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "preliminary-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree(kg):\n",
    "    with open(f\"Evaluation/splits/{kg}/train.txt\") as file:\n",
    "        data_train = file.readlines()\n",
    "    print(\"***Train*** Number of triples: \", len(data_train))\n",
    "    degree_train = defaultdict(lambda: 0)\n",
    "    for triple in data_train:\n",
    "        e1, r, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "        degree_train[e1] += 1\n",
    "        degree_train[e2] += 1\n",
    "    \n",
    "    with open(f\"Evaluation/splits/{kg}/test.txt\") as file:\n",
    "        data_test = file.readlines()\n",
    "    if not \"Merge\" in kg:\n",
    "        print(\"***Test*** Number of triples: \", len(data_test))\n",
    "    degree_test = defaultdict(lambda: 0)\n",
    "    for triple in data_test:\n",
    "        e1, r, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "        degree_test[e1] += 1\n",
    "        degree_test[e2] += 1\n",
    "    return degree_train, degree_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "central-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kg_size(kg):\n",
    "    with open(f\"Evaluation/splits/{kg}/train.txt\") as file:\n",
    "        data_train = file.readlines()\n",
    "    with open(f\"Evaluation/splits/{kg}/test.txt\") as file:\n",
    "        data_test = file.readlines()\n",
    "    E = set()\n",
    "    R = set()\n",
    "    for triple in data_train:\n",
    "        e1, r, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "        E.update({e1, e2})\n",
    "        R.add(r)\n",
    "    print(f\"***Train*** #Entities: {len(E)}, #Relations: {len(R)}\")\n",
    "    \n",
    "    E = set()\n",
    "    R = set()\n",
    "    for triple in data_test:\n",
    "        e1, r, e2 = triple.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "        E.update({e1, e2})\n",
    "        R.add(r)\n",
    "    if not \"Merge\" in kg:\n",
    "        print(f\"***Test*** #Entities: {len(E)}, #Relations: {len(R)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "animal-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Train*** Number of triples:  69667\n",
      "***Test*** Number of triples:  15374\n",
      "***Train*** #Entities: 31116, #Relations: 392\n",
      "***Test*** #Entities: 15602, #Relations: 279\n",
      "Train avg. degree:  4.477889188841753\n",
      "Test avg. degree:  1.970772977823356\n"
     ]
    }
   ],
   "source": [
    "degrees_train, degrees_test = degree(\"DBpedia\")\n",
    "kg_size(\"DBpedia\")\n",
    "train_avg_dg = np.array(list(degrees_train.values())).mean()\n",
    "test_avg_dg = np.array(list(degrees_test.values())).mean()\n",
    "print(\"Train avg. degree: \", train_avg_dg)\n",
    "print(\"Test avg. degree: \", test_avg_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "operating-plant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Train*** Number of triples:  235814\n",
      "***Test*** Number of triples:  53761\n",
      "***Train*** #Entities: 72058, #Relations: 707\n",
      "***Test*** #Entities: 41137, #Relations: 465\n",
      "Train avg. degree:  6.545116433983735\n",
      "Test avg. degree:  2.6137540413739457\n"
     ]
    }
   ],
   "source": [
    "degrees_train, degrees_test = degree(\"Wikidata\")\n",
    "kg_size(\"Wikidata\")\n",
    "train_avg_dg = np.array(list(degrees_train.values())).mean()\n",
    "test_avg_dg = np.array(list(degrees_test.values())).mean()\n",
    "print(\"Train avg. degree: \", train_avg_dg)\n",
    "print(\"Test avg. degree: \", test_avg_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "different-ballot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Train*** Number of triples:  305481\n",
      "***Train*** #Entities: 81836, #Relations: 1099\n",
      "Train avg. degree:  7.465687472505987\n"
     ]
    }
   ],
   "source": [
    "degrees_train, _ = degree(\"Merge_test_DBpedia\")\n",
    "kg_size(\"Merge_test_DBpedia\")\n",
    "train_avg_dg = np.array(list(degrees_train.values())).mean()\n",
    "print(\"Train avg. degree: \", train_avg_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-immunology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fluid-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sameAs(kg, split_type=\"train\"):\n",
    "    with open(f\"Evaluation/splits/{kg}/{split_type}.txt\") as file:\n",
    "        data = file.readlines()\n",
    "    sameAs_ents = set()\n",
    "    for line in data:\n",
    "        e1, r, e2 = line.strip(\"\\n\").strip(\" .\").split(\"\\t\")\n",
    "        if \"/entity/\" in e1:\n",
    "            sameAs_ents.add(e1)\n",
    "        if \"/entity/\" in e2:\n",
    "            sameAs_ents.add(e2)\n",
    "    print(\"Number of entities with sameAs links:\", len(sameAs_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "involved-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities with sameAs links: 22102\n"
     ]
    }
   ],
   "source": [
    "sameAs(\"DBpedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adjusted-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities with sameAs links: 10471\n"
     ]
    }
   ],
   "source": [
    "sameAs(\"DBpedia\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-nightlife",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nces",
   "language": "python",
   "name": "nces"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
