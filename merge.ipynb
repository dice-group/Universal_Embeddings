{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "featured-customer",
   "metadata": {},
   "source": [
    "We downloaded and saved [DBpedia](https://www.dbpedia.org/blog/dbpedia-snapshot-2022-09-release/) in parquet compression format (dbpedia_09_2022.parquet). We also downloaded [Wikidata](https://dumps.wikimedia.org/wikidatawiki/) as data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data from parquet to txt\n",
    "from fastparquet import ParquetFile\n",
    "filename = \"dbpedia_09_2022.parquet\"\n",
    "pf = ParquetFile(filename)\n",
    "df = pf.to_pandas()\n",
    "df.to_csv(\"dbpedia_09_2022.txt\",sep='\\t',index=False)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rel_DB ='dbpedia_09_2022.txt' # dbpedia initial file\n",
    "file_rel_write_sameas = 'sameas.txt' # file to write and store sameAs links\n",
    "file_rel_DB_without_sameas ='dbpedia_09_2022_clean.txt' # to store well-formatted triples in dbpedia\n",
    "\n",
    "dbpedia_sameas_file = open(file_rel_write_sameas, 'w', encoding='utf-8')\n",
    "dbpedia_clean_file = open(file_rel_DB_without_sameas, 'w', encoding='utf-8')\n",
    "\n",
    "def read_triple_dbp_raw(file_path):\n",
    "    num1=0\n",
    "    num2=0\n",
    "    outputsame=''\n",
    "    outputwithouterror=''\n",
    "    outputwithouterrorsameas=''\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            \n",
    "            line = line.strip('\\n').split()\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            s = line[0].lstrip('<').rstrip('>')\n",
    "            p = line[1].lstrip('<').rstrip('>')\n",
    "            o = line[2].lstrip('<').rstrip('>')\n",
    "            if 'http://www.w3.org/2002/07/owl#sameAs' in p  and 'http://www.wikidata.org/entity'  in o:\n",
    "                num1=num1+1\n",
    "                outputsame='<'+ s +'>'+'\\t'+'<'+ p +'>'+'\\t'+'<'+ o +'>\\t.\\n'\n",
    "                dbpedia_sameas_file.write(outputsame)\n",
    "            if 'http://www.w3.org/2002/07/owl#sameAs' not in p:\n",
    "                num2=num2+1\n",
    "                outputwithouterrorsameas='<'+ s +'>'+'\\t'+'<'+ p +'>'+'\\t'+'<'+ o +'>\\t.\\n'\n",
    "                dbpedia_clean_file.write(outputwithouterrorsameas)\n",
    "                \n",
    "        print(num1)\n",
    "        print(num2)\n",
    "                \n",
    "\n",
    "            \n",
    "read_triple_dbp_raw(file_rel_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete 1st line in dbpedia_09_2022_clean.txt because it contains column names, i.e., <subjetc> <relation> <object>\n",
    "sed -i '1d' dbpedia_09_2022_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-workstation",
   "metadata": {},
   "source": [
    "Define functions to merge DBpedia and Wikidata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rel_in_WIKI = 'data.txt' # relation triples in wikidata\n",
    "file_rel_in_DB = 'dbpedia_09_2022_clean.txt' # well formatted triples in dbpedia\n",
    "sameas_links = 'sameas.txt' # sameAs links\n",
    "filewrite = open('mergeentities.txt', 'a', encoding='utf-8') # file to store the fusion of dbpedia and wikidata triples via sameAs links; we replace matching\n",
    "# entity names by their corresponding names in dbpedia\n",
    "\n",
    "# function to read sameAs links and build a python dictionary\n",
    "def read_sameAs_and_build_dict(file_path):\n",
    "    dictwiki=dict()\n",
    "  # keys will be entity labels in wikidata and values their corresponding names in dbpedia\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\t.\\n').split()\n",
    "            if len(line) != 3:\n",
    "                \n",
    "                continue\n",
    "            s = line[0].lstrip('<').rstrip('>') #db\n",
    "            p = line[1].lstrip('<').rstrip('>')\n",
    "            o = line[2].lstrip('<').rstrip('>')#wiki\n",
    "            dictwiki[o]=s\n",
    "        return dictwiki\n",
    "\n",
    "\n",
    "def replace_wikidata_labels_by_corr_dbpedia_names(file_path, mapping):\n",
    "    \"\"\" Replace matching entity names by their corresponding names in dbpedia \"\"\"\n",
    "#(wiki,db)\n",
    "    output = ''\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\t.\\n').split()\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            s = line[0].lstrip('<').rstrip('>')# wiki\n",
    "            p = line[1].lstrip('<').rstrip('>')\n",
    "            o = line[2].lstrip('<').rstrip('>')# wiki\n",
    "            if s in mapping.keys() or o in mapping.keys():\n",
    "                renamesubject=mapping.get(s) #wik\n",
    "                if renamesubject is None:\n",
    "                    subject=str(s).replace(\"http://www.wikidata.org/\",\"http://embedding.cc/\")\n",
    "                else:\n",
    "                    subject=str(renamesubject).replace(\"http://dbpedia.org/\",\"http://embedding.cc/\")\n",
    "                renameobjectt=mapping.get(o)\n",
    "                if renameobjectt is None:\n",
    "                    objectt=str(o).replace(\"http://www.wikidata.org/\",\"http://embedding.cc/\")\n",
    "                else:\n",
    "                    objectt=str(renameobjectt).replace(\"http://dbpedia.org/\",\"http://embedding.cc/\")\n",
    "            else:\n",
    "                subject=str(s).replace(\"http://dbpedia.org/\",\"http://embedding.cc/\")\n",
    "                objectt=str(o).replace(\"http://dbpedia.org/\",\"http://embedding.cc/\")\n",
    "            output = '<'+ subject +'>'+'\\t'+'<'+ str(p) +'>'+'\\t'+'<'+ objectt +'>\\t.\\n'\n",
    "            filewrite.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-debate",
   "metadata": {},
   "source": [
    "First write Dbpedia triples into the merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_write_dbpedia_triples_to_merge_kg(dbpedia_file):\n",
    "    with open(dbpedia_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            filewrite.write(line) # write into the merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first write dbpedia triples into the merge file. wikidata triples will be added below using Algorithm 1 of our paper.\n",
    "read_write_dbpedia_triples_to_merge_kg(file_rel_in_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-evaluation",
   "metadata": {},
   "source": [
    "Now build the mapping dictionary using sameAs links then add wikidata triples into the merge file using Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_to_dbpedia = read_sameAs_and_build_dict(sameas_links) # sameAs links\n",
    "replace_wikidata_labels_by_corr_dbpedia_names(file_rel_in_WIKI, wikidata_to_dbpedia) # add wikidata triples\n",
    "filewrite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-delaware",
   "metadata": {},
   "source": [
    "Data statistics and average degree in KGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate the triples\n",
    "def readline_count(file_name):\n",
    "      with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        num=0\n",
    "        for line in file:\n",
    "            line = line.strip('\\t.\\n').split()\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            num=num+1\n",
    "        print(num)\n",
    "readline_count('mergeentities.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the average degree\n",
    "from collections import defaultdict\n",
    "def degree(file_name):\n",
    "    kg_degree = defaultdict(lambda: 0)\n",
    "    with open(file_name) as file:\n",
    "        #data = file.readlines()\n",
    "       # print(\"***Train*** Number of triples: \", len(data))\n",
    "     for triple in file:\n",
    "            triple = triple.strip('\\t.\\n').split()\n",
    "            e1 = triple[0].lstrip('<').rstrip('>')\n",
    "            r = triple[1].lstrip('<').rstrip('>')\n",
    "            e2 = triple[2].lstrip('<').rstrip('>')\n",
    "            kg_degree[e1] += 1\n",
    "            kg_degree[e2] += 1\n",
    "    return kg_degree\n",
    "\n",
    "import numpy as np\n",
    "degrees = degree(\"mergeentities.txt\") ## Replace by KG file name here. In our case, you will do it for 3 KGs: mergeentities.txt, dbpedia_09_2022_clean.txt, data.txt (wikidata kg)\n",
    "print(\"Avg. degree:\", np.array(list(degrees.values())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kg_size(file_name):\n",
    "    E = set() # entities\n",
    "    R = set() # relations\n",
    "    with open(file_name) as file:\n",
    "        #data = file.readlines()\n",
    "        for triple in file:\n",
    "            triple = triple.strip('\\t.\\n').split()\n",
    "           \n",
    "            e1 = triple[0].lstrip('<').rstrip('>')# db\n",
    "            r = triple[1].lstrip('<').rstrip('>')# db\n",
    "            e2 = triple[2].lstrip('<').rstrip('>')# db\n",
    "            \n",
    "            E.update({e1, e2})\n",
    "            R.add(r)\n",
    "    print(f\"#Entities: {len(E)}, #Relations: {len(R)}\")\n",
    "kg_size('data.txt')\n",
    "kg_size('mergeentities.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "cl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
